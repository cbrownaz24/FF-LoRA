Unsupported operator aten::embedding encountered 1 time(s)
Unsupported operator aten::add encountered 121 time(s)
Unsupported operator aten::cos encountered 1 time(s)
Unsupported operator aten::sin encountered 1 time(s)
Unsupported operator aten::mul encountered 122 time(s)
Unsupported operator aten::neg encountered 48 time(s)
Unsupported operator aten::scaled_dot_product_attention encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
base_model.model.gpt_neox.layers.0.attention.attention_dropout, base_model.model.gpt_neox.layers.0.attention.rotary_emb, base_model.model.gpt_neox.layers.1.attention.attention_dropout, base_model.model.gpt_neox.layers.1.attention.rotary_emb, base_model.model.gpt_neox.layers.10.attention.attention_dropout, base_model.model.gpt_neox.layers.10.attention.rotary_emb, base_model.model.gpt_neox.layers.11.attention.attention_dropout, base_model.model.gpt_neox.layers.11.attention.rotary_emb, base_model.model.gpt_neox.layers.12.attention.attention_dropout, base_model.model.gpt_neox.layers.12.attention.rotary_emb, base_model.model.gpt_neox.layers.13.attention.attention_dropout, base_model.model.gpt_neox.layers.13.attention.rotary_emb, base_model.model.gpt_neox.layers.14.attention.attention_dropout, base_model.model.gpt_neox.layers.14.attention.rotary_emb, base_model.model.gpt_neox.layers.15.attention.attention_dropout, base_model.model.gpt_neox.layers.15.attention.rotary_emb, base_model.model.gpt_neox.layers.16.attention.attention_dropout, base_model.model.gpt_neox.layers.16.attention.rotary_emb, base_model.model.gpt_neox.layers.17.attention.attention_dropout, base_model.model.gpt_neox.layers.17.attention.rotary_emb, base_model.model.gpt_neox.layers.18.attention.attention_dropout, base_model.model.gpt_neox.layers.18.attention.rotary_emb, base_model.model.gpt_neox.layers.19.attention.attention_dropout, base_model.model.gpt_neox.layers.19.attention.rotary_emb, base_model.model.gpt_neox.layers.2.attention.attention_dropout, base_model.model.gpt_neox.layers.2.attention.rotary_emb, base_model.model.gpt_neox.layers.20.attention.attention_dropout, base_model.model.gpt_neox.layers.20.attention.rotary_emb, base_model.model.gpt_neox.layers.21.attention.attention_dropout, base_model.model.gpt_neox.layers.21.attention.rotary_emb, base_model.model.gpt_neox.layers.22.attention.attention_dropout, base_model.model.gpt_neox.layers.22.attention.rotary_emb, base_model.model.gpt_neox.layers.23.attention.attention_dropout, base_model.model.gpt_neox.layers.23.attention.rotary_emb, base_model.model.gpt_neox.layers.3.attention.attention_dropout, base_model.model.gpt_neox.layers.3.attention.rotary_emb, base_model.model.gpt_neox.layers.4.attention.attention_dropout, base_model.model.gpt_neox.layers.4.attention.rotary_emb, base_model.model.gpt_neox.layers.5.attention.attention_dropout, base_model.model.gpt_neox.layers.5.attention.rotary_emb, base_model.model.gpt_neox.layers.6.attention.attention_dropout, base_model.model.gpt_neox.layers.6.attention.rotary_emb, base_model.model.gpt_neox.layers.7.attention.attention_dropout, base_model.model.gpt_neox.layers.7.attention.rotary_emb, base_model.model.gpt_neox.layers.8.attention.attention_dropout, base_model.model.gpt_neox.layers.8.attention.rotary_emb, base_model.model.gpt_neox.layers.9.attention.attention_dropout, base_model.model.gpt_neox.layers.9.attention.rotary_emb
Traceback (most recent call last):
  File "/home/connorb/FF-LoRA/main.py", line 429, in <module>
    vanilla_final_loss, vanilla_test_curve, vanilla_time, vanilla_flops = vanilla_train(
  File "/home/connorb/FF-LoRA/main.py", line 258, in vanilla_train
    avg_test_loss = compute_avg_test_loss(model, test_dataloader, device).item()
  File "/home/connorb/FF-LoRA/main.py", line 183, in compute_avg_test_loss
    test_batch = {k: v.to(device) for k, v in test_batch.items()}
  File "/home/connorb/FF-LoRA/main.py", line 183, in <dictcomp>
    test_batch = {k: v.to(device) for k, v in test_batch.items()}
KeyboardInterrupt
